{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Pytorch\n",
    "\n",
    "Phai Phongthiengtham"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Math behind linear regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the true data generating process is: $y = wx$, with the true value of w: $w_{true} = 2$\n",
    "\n",
    "For observation $i$, denote $\\hat{y_i} = wx_i$ to be predicted value of $y_i$. Denote $N$ to be number of observations. Then the mean squared error loss function is:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{n}\\sum_{i=1}^N(\\hat{y_i} - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^N(w\\hat{x_i} - y_i)^2 \\nonumber\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{n}\\sum_{i=1}^N\\Bigl(2x_i(\\hat{y_i} - y_i)\\Bigl) \\nonumber\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "This is how to implement in python as vectors:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w} = mean\\Bigl(2\\vec{x} \\cdot (\\vec{\\hat{y}} - \\vec{y}) \\Bigl)\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using numpy\n",
    "\n",
    "We will start with using numpy for the whole training process.\n",
    "\n",
    "Given the current value of parameter(s), the training has the following steps: \n",
    "1. Make prediction: also known as forward propagation\n",
    "2. Compute loss function\n",
    "3. Compute gradient: also known as backward propagation\n",
    "4. Adjust the value of parameter(s)\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "w_{updated} = w_{old} - \\eta\\frac{\\partial L}{\\partial w_{old}} \\nonumber\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "where $\\eta$ refers to learning rate. The gradient $\\frac{\\partial L}{\\partial w}$ is calculated from equation (1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 2.200, loss = 7.50000000\n",
      "epoch 2: w = 1.960, loss = 0.30000019\n",
      "epoch 3: w = 2.008, loss = 0.01200006\n",
      "epoch 4: w = 1.998, loss = 0.00048001\n",
      "epoch 5: w = 2.000, loss = 0.00001920\n",
      "epoch 6: w = 2.000, loss = 0.00000077\n",
      "epoch 7: w = 2.000, loss = 0.00000003\n",
      "epoch 8: w = 2.000, loss = 0.00000000\n",
      "epoch 9: w = 2.000, loss = 0.00000000\n",
      "epoch 10: w = 2.000, loss = 0.00000000\n",
      "epoch 11: w = 2.000, loss = 0.00000000\n",
      "epoch 12: w = 2.000, loss = 0.00000000\n",
      "epoch 13: w = 2.000, loss = 0.00000000\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32) # y = w*x where true w = 2 \n",
    "\n",
    "def forward(x): # 1. make prediction\n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_predicted): # 2. calculate loss\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "def gradient(x, y, y_predicted): # 3. calculate gradient\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "w = 1.0 # initialize w\n",
    "learning_rate = 0.02 # how fast the parameters update (hyperparameter)\n",
    "n_iters = 20 # number of iterations\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradient\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw # new w = old w - (learning_rate * dw)\n",
    "\n",
    "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using torch to compute gradient\n",
    "\n",
    "Next, we will use torch to automatically compute gradient. This is needed because we cannot compute the formula for gredient with the majority of deep learning models.\n",
    "\n",
    "Reminders:\n",
    "- We need to tell torch to prepare to compute gradient by setting: ```requires_grad=True``` when initializing tensors.\n",
    "- If we set ```requires_grad=True```, we can wrap with ```torch.no_grad():``` when we do **NOT** want to compute gradient.\n",
    "- Once finish, we have to empty the gradient by setting ```.grad.zero_()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.750, loss = 7.50000000\n",
      "epoch 2: w = 1.938, loss = 0.46875000\n",
      "epoch 3: w = 1.984, loss = 0.02929688\n",
      "epoch 4: w = 1.996, loss = 0.00183105\n",
      "epoch 5: w = 1.999, loss = 0.00011444\n",
      "epoch 6: w = 2.000, loss = 0.00000715\n",
      "epoch 7: w = 2.000, loss = 0.00000045\n",
      "epoch 8: w = 2.000, loss = 0.00000003\n",
      "epoch 9: w = 2.000, loss = 0.00000000\n",
      "epoch 10: w = 2.000, loss = 0.00000000\n",
      "epoch 11: w = 2.000, loss = 0.00000000\n",
      "epoch 12: w = 2.000, loss = 0.00000000\n",
      "epoch 13: w = 2.000, loss = 0.00000000\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32) # [UPDATED] instead of numpy array, now we use torch tensor\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32) # [UPDATED] instead of numpy array, now we use torch tensor\n",
    "\n",
    "def forward(x): # 1. make prediction\n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_predicted): # # 2. calculate loss\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# [UPDATED] no longer need this, we will use built-in torch function to compute gradient  \n",
    "#def gradient(x, y, y_predicted): # 3. calculate gradient\n",
    "    #return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "# [UPDATED] instead of numpy array, now we use torch tensor - REMEMBER \"requires_grad=True\" is required!\n",
    "w = torch.tensor(1.0, dtype=torch.float32, requires_grad=True) # initialize w\n",
    "\n",
    "learning_rate = 0.05 # how fast the parameters update (hyperparameter)\n",
    "n_iters = 20 # number of iterations\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradient\n",
    "    #dw = gradient(X, Y, y_pred)\n",
    "    l.backward() # [UPDATED] use torch to compute gradient with respect to w \n",
    "    \n",
    "    # update weights\n",
    "    with torch.no_grad(): \n",
    "        # [UPDATED] REMEMBER \"torch.no_grad()\" is required! We do not want to compute gradient here \n",
    "        w -= learning_rate * w.grad \n",
    "\n",
    "    # empty gradient vector\n",
    "    w.grad.zero_() # [UPDATED] REMEMBER \".grad.zero_()\" is required! We are done in this epoch.\n",
    "\n",
    "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using torch to compute gradient, loss and update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.750, loss = 7.50000000\n",
      "epoch 2: w = 1.938, loss = 0.46875000\n",
      "epoch 3: w = 1.984, loss = 0.02929688\n",
      "epoch 4: w = 1.996, loss = 0.00183105\n",
      "epoch 5: w = 1.999, loss = 0.00011444\n",
      "epoch 6: w = 2.000, loss = 0.00000715\n",
      "epoch 7: w = 2.000, loss = 0.00000045\n",
      "epoch 8: w = 2.000, loss = 0.00000003\n",
      "epoch 9: w = 2.000, loss = 0.00000000\n",
      "epoch 10: w = 2.000, loss = 0.00000000\n",
      "epoch 11: w = 2.000, loss = 0.00000000\n",
      "epoch 12: w = 2.000, loss = 0.00000000\n",
      "epoch 13: w = 2.000, loss = 0.00000000\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# [UPDATED] no longer need this, we will use built-in torch function to compute loss \n",
    "#def loss(y, y_predicted):\n",
    "    #return ((y_predicted-y)**2).mean()\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "w = torch.tensor(1.0, dtype=torch.float32, requires_grad=True) # initialize w\n",
    "\n",
    "# [UPDATED] we will use torch optimizer module for updating weights. lr is learning rate \n",
    "optimizer = torch.optim.SGD([w], lr=0.05)\n",
    "\n",
    "n_iters = 20 # number of iterations\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradient\n",
    "    l.backward()\n",
    "\n",
    "    # [UPDATED] no longer need this, we will use built-in torch function to update weights\n",
    "    # update weights\n",
    "    #with torch.no_grad(): \n",
    "        #w -= learning_rate * w.grad\n",
    "    optimizer.step() \n",
    "\n",
    "    # empty gradient vector\n",
    "    optimizer.zero_grad() # [UPDATED] still need this, but we do this on the optimizer instead of w.\n",
    "\n",
    "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using torch to do everything (compute gradient and loss, update weights, and make prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.702, loss = 34.29173279\n",
      "epoch 2: w = 1.966, loss = 0.93370378\n",
      "epoch 3: w = 2.009, loss = 0.02585376\n",
      "epoch 4: w = 2.016, loss = 0.00113357\n",
      "epoch 5: w = 2.017, loss = 0.00044801\n",
      "epoch 6: w = 2.017, loss = 0.00041696\n",
      "epoch 7: w = 2.016, loss = 0.00040409\n",
      "epoch 8: w = 2.016, loss = 0.00039207\n",
      "epoch 9: w = 2.016, loss = 0.00038042\n",
      "epoch 10: w = 2.016, loss = 0.00036911\n",
      "epoch 11: w = 2.016, loss = 0.00035814\n",
      "epoch 12: w = 2.015, loss = 0.00034750\n",
      "epoch 13: w = 2.015, loss = 0.00033718\n",
      "epoch 14: w = 2.015, loss = 0.00032716\n",
      "epoch 15: w = 2.015, loss = 0.00031744\n",
      "epoch 16: w = 2.014, loss = 0.00030801\n",
      "epoch 17: w = 2.014, loss = 0.00029886\n",
      "epoch 18: w = 2.014, loss = 0.00028997\n",
      "epoch 19: w = 2.014, loss = 0.00028136\n",
      "epoch 20: w = 2.014, loss = 0.00027300\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # [UPDATED] torch expects a different data shape\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) # [UPDATED] torch expects a different data shape\n",
    "\n",
    "# [UPDATED] no longer need this, we will use built-in torch function.\n",
    "#def forward(x):\n",
    "    #return w * x\n",
    "\n",
    "# [UPDATED] We will use built-in Linear model from torch\n",
    "n_samples = 4\n",
    "n_features = 1\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = torch.nn.Linear(input_size, output_size)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# [UPDATED] no longer need this, this is already in torch Linear model\n",
    "#w = torch.tensor(1.0, dtype=torch.float32, requires_grad=True) # initialize w\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "n_iters = 20 # number of iterations\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = model(X) # [UPDATED] Now we call the model directly!\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradient\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step() \n",
    "\n",
    "    # empty gradient vector\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    [w, b] = model.parameters() # [UPDATED] Now we have to unpack parameters\n",
    "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
