{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing code: Clean unstructured Burning Glass data\n",
    "\n",
    "Phai Phongthiengtham: IBM CAO 2018 summer intern\n",
    " \n",
    "***\n",
    "\n",
    "This notebook demonstrates how to pre-process text, extract task measures, and prepare input for word2vec model.\n",
    "\n",
    "- Variable \"BGTJobId\" is a identifier -- to be merged with processed unstructured data\n",
    "\n",
    "Note:\n",
    "- Make sure to follow up with the package updates and whether any syntax has to be changed.\n",
    "- The scikit-learn module, in particular, does changes syntax quite often!\n",
    "\n",
    "IMPORTANT:\n",
    "- The sample input file in this notebook is from MIT, and the format of the recently purchased version by IBM could be different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyldavis\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 609kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: wheel>=0.23.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyldavis)\n",
      "Requirement not upgraded as not directly required: numpy>=1.9.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyldavis)\n",
      "Requirement not upgraded as not directly required: scipy>=0.18.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyldavis)\n",
      "Requirement not upgraded as not directly required: pandas>=0.17.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyldavis)\n",
      "Collecting joblib>=0.8.4 (from pyldavis)\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/26/317725ffd9e8e8c0eb4b2fc77614f52045ddfc1c5026387fbefef9050eec/joblib-0.12.2-py2.py3-none-any.whl (269kB)\n",
      "\u001b[K    100% |████████████████████████████████| 276kB 3.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: jinja2>=2.7.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyldavis)\n",
      "Requirement not upgraded as not directly required: numexpr in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyldavis)\n",
      "Requirement not upgraded as not directly required: pytest in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyldavis)\n",
      "Requirement not upgraded as not directly required: future in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyldavis)\n",
      "Collecting funcy (from pyldavis)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/da/03e74264b3289be52a54733960d2e1b2dbe5190b64c8489f63c2f072bfc0/funcy-1.10.3-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: python-dateutil>=2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas>=0.17.0->pyldavis)\n",
      "Requirement not upgraded as not directly required: pytz>=2011k in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas>=0.17.0->pyldavis)\n",
      "Requirement not upgraded as not directly required: MarkupSafe>=0.23 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from jinja2>=2.7.2->pyldavis)\n",
      "Requirement not upgraded as not directly required: py>=1.4.33 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pytest->pyldavis)\n",
      "Requirement not upgraded as not directly required: setuptools in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pytest->pyldavis)\n",
      "Requirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python-dateutil>=2->pandas>=0.17.0->pyldavis)\n",
      "Building wheels for collected packages: pyldavis\n",
      "  Running setup.py bdist_wheel for pyldavis ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
      "Successfully built pyldavis\n",
      "Installing collected packages: joblib, funcy, pyldavis\n",
      "Successfully installed funcy-1.10.3 joblib-0.12.2 pyldavis-2.1.2\n",
      "Collecting spacy\n",
      "  Downloading https://files.pythonhosted.org/packages/24/de/ac14cd453c98656d6738a5669f96a4ac7f668493d5e6b78227ac933c5fd4/spacy-2.0.12.tar.gz (22.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 22.0MB 44kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: numpy>=1.7 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy)\n",
      "Collecting murmurhash<0.29,>=0.28 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/31/c8c1ecafa44db30579c8c457ac7a0f819e8b1dbc3e58308394fff5ff9ba7/murmurhash-0.28.0.tar.gz\n",
      "Collecting cymem<1.32,>=1.30 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/43/39372a0bc24d336dc88b87262c30f09d0a2c759f32a2965f90fb56da46f1/cymem-1.31.2-cp35-cp35m-manylinux1_x86_64.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/be/fc/09684555ce0ee7086675e6be698e4efeb6d9b315fd5aa96bed347572282b/preshed-1.0.1.tar.gz (112kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<6.11.0,>=6.10.3 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 800kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting ujson>=1.35 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: dill<0.3,>=0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy)\n",
      "Collecting regex==2017.4.5 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K    100% |████████████████████████████████| 604kB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: requests<3.0.0,>=2.13.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy)\n",
      "Collecting msgpack<1.0.0,>=0.5.6 (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/72/5a01d2a6a894e7f6966b0038445c748d7a16754cceb0e988699269d8152a/msgpack-0.5.6-cp35-cp35m-manylinux1_x86_64.whl (309kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting msgpack-numpy<1.0.0,>=0.4.1 (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/84/09/fc890664a7a1dd0a88f46c93fb9340d0a27a69e82095a4a54aef2ed94a6d/msgpack_numpy-0.4.3.1-py2.py3-none-any.whl\n",
      "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz (443kB)\n",
      "\u001b[K    100% |████████████████████████████████| 450kB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: wrapt<1.11.0,>=1.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "Requirement not upgraded as not directly required: tqdm<5.0.0,>=4.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "Requirement not upgraded as not directly required: six<2.0.0,>=1.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy)\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement not upgraded as not directly required: toolz>=0.8.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy)\n",
      "Building wheels for collected packages: spacy, murmurhash, preshed, thinc, ujson, regex, cytoolz\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/60/0b/bb/7c2e28db574dbb2358176934eddd32a1c5f838ba0bc23eaaab\n",
      "  Running setup.py bdist_wheel for murmurhash ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/b8/94/a4/f69f8664cdc1098603df44771b7fec5fd1b3d8364cdd83f512\n",
      "  Running setup.py bdist_wheel for preshed ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/ca/e5/8b/73706d7232da301838e0bc564367a2f7b2fc8f834228fc8a4b\n",
      "  Running setup.py bdist_wheel for thinc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/db/bc/e1/9b321b6b203288cf636a56e668ed5700076af4ed66062278ca\n",
      "  Running setup.py bdist_wheel for ujson ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "  Running setup.py bdist_wheel for cytoolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/88/f3/11/9817b001e59ab04889e8cffcbd9087e2e2155b9ebecfc8dd38\n",
      "Successfully built spacy murmurhash preshed thinc ujson regex cytoolz\n",
      "Installing collected packages: murmurhash, cymem, preshed, msgpack, msgpack-numpy, cytoolz, plac, thinc, ujson, regex, spacy\n",
      "  Found existing installation: cytoolz 0.8.2\n",
      "    Uninstalling cytoolz-0.8.2:\n",
      "      Successfully uninstalled cytoolz-0.8.2\n",
      "Successfully installed cymem-1.31.2 cytoolz-0.9.0.1 msgpack-0.5.6 msgpack-numpy-0.4.3.1 murmurhash-0.28.0 plac-0.9.6 preshed-1.0.1 regex-2017.4.5 spacy-2.0.12 thinc-6.10.3 ujson-1.35\n",
      "Collecting scikit-learn\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/e2/a1e254a4a4598588d4fe88b45ab88a226c289ecfd0f6c90474eb6a9ea6b3/scikit_learn-0.19.2-cp35-cp35m-manylinux1_x86_64.whl (4.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.9MB 202kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.19.1\n",
      "    Uninstalling scikit-learn-0.19.1:\n",
      "      Successfully uninstalled scikit-learn-0.19.1\n",
      "Successfully installed scikit-learn-0.19.2\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 26kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: spacy>=2.0.0a18 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: plac<1.0.0,>=0.9.6 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: numpy>=1.7 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: cymem<1.32,>=1.30 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: thinc<6.11.0,>=6.10.3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: ujson>=1.35 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: preshed<2.0.0,>=1.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: dill<0.3,>=0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: requests<3.0.0,>=2.13.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: murmurhash<0.29,>=0.28 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: regex==2017.4.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: msgpack<1.0.0,>=0.5.6 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: wrapt<1.11.0,>=1.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: cytoolz<0.10,>=0.9.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: six<2.0.0,>=1.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: tqdm<5.0.0,>=4.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: msgpack-numpy<1.0.0,>=0.4.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.3->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Requirement not upgraded as not directly required: toolz>=0.8.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy>=2.0.0a18->en-core-web-sm==2.0.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Running setup.py bdist_wheel for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/54/7c/d8/f86364af8fbba7258e14adae115f18dd2c91552406edc3fdaa\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.0.0\n",
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 738kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "Successfully installed nltk-3.3\n",
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/d7/b6e1f23eb5ded71c0e724e0f60a08f96853c6503a51b7cc23c3761aa9ec4/gensim-3.5.0-cp35-cp35m-manylinux1_x86_64.whl (23.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 23.5MB 41kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: numpy>=1.11.3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from gensim)\n",
      "Requirement not upgraded as not directly required: scipy>=0.18.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from gensim)\n",
      "Requirement not upgraded as not directly required: six>=1.5.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from gensim)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/3d/5f3a9a296d0ba8e00e263a8dee76762076b9eb5ddc254ccaa834651c8d65/smart_open-1.6.0.tar.gz\n",
      "Requirement not upgraded as not directly required: boto>=2.32 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement not upgraded as not directly required: requests in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: boto3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: botocore<1.8.0,>=1.7.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: s3transfer<0.2.0,>=0.1.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from botocore<1.8.0,>=1.7.0->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from botocore<1.8.0,>=1.7.0->boto3->smart-open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/73/f1/9b/ccf93d4ba073b6f79b1ed9df68ab5ce048d8136d0efcf90b30\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, smart-open, gensim\n",
      "Successfully installed bz2file-0.98 gensim-3.5.0 smart-open-1.6.0\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/dsxuser/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U pyldavis\n",
    "!pip install -U spacy\n",
    "!pip install -U scikit-learn\n",
    "!pip install -U https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
    "!pip install -U nltk\n",
    "!pip install -U gensim\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, re, os, json, sys, csv, time, datetime, types\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import operator, curl\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "from scipy import spatial\n",
    "from pprint import pprint\n",
    "from project_lib import Project\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, KeyedVectors\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# scacy\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger','ner'] )\n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "def __iter__(self): return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials for load and save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client_c134506f72ab49d1a232818751853c8a = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='28lR3Um-L0FbgIYW7qqJQ1ezn2VXdjWlZJV3XtoYUA4u',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project = Project(project_id='fbb58841-9ee4-4f0d-9784-ea4c5475261d', project_access_token='p-ada0289c1d7acc5fa217767ec4ada2573bdf20bd')\n",
    "pc = project.project_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that imports the raw file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_BurningGlass(filename):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        body_file = client_c134506f72ab49d1a232818751853c8a.get_object(Bucket='mlforeconomy-donotdelete-pr-o5jigrnf6kwxg5', Key=filename)['Body']\n",
    "        if not hasattr(body_file, \"__iter__\"): \n",
    "            body_file.__iter__ = types.MethodType( __iter__, body_file )\n",
    "\n",
    "        df = pd.read_csv(body_file, sep = '\\t', header = 0, dtype = object, encoding = 'utf-8')\n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        \n",
    "        body_file = client_c134506f72ab49d1a232818751853c8a.get_object(Bucket='mlforeconomy-donotdelete-pr-o5jigrnf6kwxg5', Key=filename)['Body']\n",
    "        if not hasattr(body_file, \"__iter__\"): \n",
    "            body_file.__iter__ = types.MethodType( __iter__, body_file )\n",
    "\n",
    "        df = pd.read_csv(body_file, sep = '\\t', header = 0, dtype = object, encoding = 'latin-1')\n",
    "    \n",
    "    print(filename + ' : ' + str(len(df)) + ' records' )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define clusters of task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr_analytic = ['researching','analyzing', 'evaluating', 'planning', 'designing', 'sketching', 'research', 'analyze', 'evaluate', \n",
    "               'plan', 'design', 'sketch','devising rule', 'interpreting rule', 'budgeting', 'lettering', 'stylized', 'implementing', \n",
    "               'evaluation', 'developing', 'plans', 'analyse', 'architecture', 'installing', 'determining', 'freehand', 'sketcher', \n",
    "               'compiling', 'deign', 'rsrch', 'devising', 'administering', 'researcher', 'modifying', 'mechanicals', 'desi', 'assessing', \n",
    "               'correlate', 'synthesizing', 'implement', 'validate', 'rendering', 'defining', 'sketchbook', 'conceptualizing', 'creating', \n",
    "               'validating', 'synthesize', 'analysing', 'renderer', 'resolving', 'monitoring', 'reviewing', 'designs', 'interpreting', 'rule', \n",
    "               'fettering', 'summarize', 'researching', 'recommending', 'monitor', 'collecting', 'configuring', 'analyzing', 'identify', \n",
    "               'identifying', 'compile', 'develop', 'assess', 'formulating', 'layouts', 'constructing', 'investigating', 'architecting', \n",
    "               'prepare', 'formulate', 'evaluating', 'interpret', 'renderings', 'documenting', 'define', 'mock', 'gathering', 'recommend', \n",
    "               'evaluates', 'pylon', 'summarizing', 'illustration', 'artistic', 'determine', 'coordinate', 'dylan', 'gather', 'airbrush', \n",
    "               'analyzes', 'designer', 'analysis', 'illustrating', 'sketches', 'deploying', 'examine', 'designing', 'resign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr_interactive = ['negotiating', 'lobbying', 'coordinating', 'organizing', 'teaching', 'selling', 'buying','advising', 'advertising', \n",
    "                  'entertaining', 'presenting' , 'managing','negotiate', 'lobby','coordinate', 'organize', 'sell', 'purchase', 'advise', \n",
    "                  'advertise', 'entertain', 'presentation', 'presentations', 'tising', 'legislators', 'assisting', 'manage', 'lobbyist', \n",
    "                  'educating', 'educate', 'azov', 'articulating', 'directing', 'proposals', 'seil', 'developing', 'guying', 'vertislng', \n",
    "                  'advertlslag', 'iobby', 'structuring', 'sales', 'facilitating', 'execute', 'supporting', 'advtsg', 'administering', \n",
    "                  'negotiations', 'advt', 'lobbv', 'executing', 'explaining', 'odv', 'informative', 'advtg', 'lobbyists', 'advertlsln', \n",
    "                  'adv', 'briefings', 'coordinates', 'lobb', 'congressional', 'advert', 'delightful', 'seli', 'presenting', 'intergovernmental', \n",
    "                  'reviewing', 'advises', 'communicating', 'soiling', 'adverting', 'arranging', 'adverts', 'adver', 'vtg', \n",
    "                  'oversee', 'adverhslng', 'coordination', 'publicity', 'grassroots', 'legislative', 'academic', 'leaching', \n",
    "                  'advfg', 'adve', 'media', 'marketing', 'advertiser', 'irreverent', 'prioritize', 'teething', 'prioritizing', \n",
    "                  'adverllslng', 'overseeing', 'negotiates', 'initiate', 'supervise', 'ertislng', 'nightlife', 'negotiation', \n",
    "                  'kdv', 'leeching', 'selim', 'baying', 'evaluate', 'consult', 'preparing', 'supervising', 'negotiating', \n",
    "                  'facilitate', 'administer', 'presents', 'inform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_cognitive = ['calculating', 'bookkeeping', 'correcting', 'measuring','calculate', 'corrections', 'measurement','gauging', 'modifications', \n",
    "               'measurements', 'bkkpo', 'bkkp', 'isolating', 'bookkpg', 'micrometer', 'correcting', 'payroll', 'measurement', 'measuring', \n",
    "               'ajp', 'calculation', 'calipers', 'corrects', 'calculations', 'bkkpg', 'correction', 'bluing', 'revisions', 'dkkpg', \n",
    "               'bookkeeper', 'adjustments', 'reconcile', 'calculate', 'calculates', 'stenography', 'bkkpng', 'bkkping', 'compute', 'resolving', \n",
    "               'clerical', 'billing', 'fixing', 'calculating', 'measure', 'rectifying', 'beekeeping', 'gauges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_manual = ['operating', 'controlling', 'equipping','operate', 'control', 'equip', 'equipment''quip', 'eqp', 'troll', 'instruments', 'minimizing', \n",
    "            'eouip', 'eaulp', 'equlp', 'ntrol', 'eguip', 'quid', 'equlo', 'eoulp', 'equtp', 'equip', 'gulp', 'machines', 'apparatus', 'uip', 'equipage', \n",
    "            'eqpmt', 'controls', 'sterilizers', 'controlling', 'eauio', 'eqpt', 'engulfment', 'eauip', 'equipment', 'conhol', 'devices', 'ulp', 'control', \n",
    "            'eq', 'equlpt', 'instrumentation', 'equ', 'equio', 'operation', 'equlpmt', 'operating', 'machinery', 'epuip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr_manual = ['repairing', 'renovating', 'restoring', 'accommodating','repair', 'renovate', 'restore', 'service', 'accommodation', 'accommodate',\n",
    "             'accommodations', 'inspecting', 'calibrating', 'accomodate', 'overhaul', 'rebuilding', 'serves', 'installing', 'restoring', 'serve', \n",
    "             'installation', 'diagnosing', 'reassembling', 'mechanic', 'repairs', 'overhauling', 'accomodation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean unstructured text data\n",
    "- Only tokenize, NOT lemmatize  \n",
    "- https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# string replace\n",
    "def cleanup(text):\n",
    "    if text == '': # allows for possibility of being empty \n",
    "        output = ''\n",
    "    else:\n",
    "        text = text.replace(\"'s\", \" \")\n",
    "        text = text.replace(\"n't\", \" not \")\n",
    "        text = text.replace(\"'ve\", \" have \")\n",
    "        text = text.replace(\"'re\", \" are \")\n",
    "        text = text.replace(\"'m\",\"  am \")\n",
    "        text = text.replace(\"'ll\",\"  will \")\n",
    "        text = text.replace(\"-\",\" \")\n",
    "        text = text.replace(\"/\",\" \")\n",
    "        text = text.replace(\"(\",\" \")\n",
    "        text = text.replace(\")\",\" \")\n",
    "        text = re.sub(r'[^A-Za-z ]', '', text) #remove all characters that are not A-Z, a-z or 0-9\n",
    "        output = ' '.join([w for w in re.split(' ',text) if not w=='']) #remove extra spaces \n",
    "    return output  \n",
    "\n",
    "# pre-process text\n",
    "def main_preprocess(text):\n",
    "    text = str(text) # make sure the input is actually string\n",
    "    text = ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "    if text == '': # allows for possibility of being empty \n",
    "        output = ''\n",
    "    else:\n",
    "        tokens = [w.text for w in nlp(cleanup(text))] # cleanup and tokenize\n",
    "        output = ' '.join([w.lower() for w in tokens if not w==''])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on JobText_US_01-04-2016.txt\n",
      "JobText_US_01-04-2016.txt : 82779 records\n",
      "working on JobText_US_01-05-2016.txt\n",
      "JobText_US_01-05-2016.txt : 104775 records\n",
      "working on JobText_US_01-06-2016.txt\n",
      "JobText_US_01-06-2016.txt : 153132 records\n",
      "---DONE---\n"
     ]
    }
   ],
   "source": [
    "BG_file_numbers = ['01-04-2016', '01-05-2016', '01-06-2016']\n",
    "\n",
    "for num in BG_file_numbers:\n",
    "    \n",
    "    filename = 'JobText_US_' + num + '.txt' # example: 'JobText_US_01-04-2016.txt'\n",
    "    \n",
    "    print('working on ' + filename)\n",
    "    df = import_BurningGlass(filename)\n",
    "    df = df[['BGTJobId','JobId','JobText']]\n",
    "    \n",
    "    if len(df) > 10000:\n",
    "        df = df.sample(10000) # sample 10000 ads\n",
    "    \n",
    "    # pre-process\n",
    "    df['CleanText'] = df['JobText'].apply(lambda x: main_preprocess(x))\n",
    "    \n",
    "    # extract task\n",
    "    df['t_nr_analytic'] = df['CleanText'].apply(lambda x: len(re.findall('|'.join(['\\\\b' + w + '\\\\b' for w in nr_analytic]), x)))\n",
    "    df['t_nr_interactive'] = df['CleanText'].apply(lambda x: len(re.findall('|'.join(['\\\\b' + w + '\\\\b' for w in nr_interactive]), x)))\n",
    "    df['t_r_cognitive'] = df['CleanText'].apply(lambda x: len(re.findall('|'.join(['\\\\b' + w + '\\\\b' for w in r_cognitive]), x)))\n",
    "    df['t_r_manual'] = df['CleanText'].apply(lambda x: len(re.findall('|'.join(['\\\\b' + w + '\\\\b' for w in r_manual]), x)))\n",
    "    df['t_nr_manual'] = df['CleanText'].apply(lambda x: len(re.findall('|'.join(['\\\\b' + w + '\\\\b' for w in nr_manual]), x)))\n",
    "    \n",
    "    # export sample files\n",
    "    df_sample = df[['BGTJobId', 'JobId', 'JobText', 'CleanText']].sample(100)\n",
    "    output_filename = 'BG_sampled.csv'\n",
    "    project.save_data(output_filename, df_sample.to_csv(index=False), overwrite=True)\n",
    "\n",
    "    # export task\n",
    "    df_task = df[['BGTJobId', 'JobId', 't_nr_analytic', 't_nr_interactive', 't_r_cognitive', 't_r_manual', 't_nr_manual']]\n",
    "    task_output_filename = 'BG_task_' + num + '.csv'\n",
    "    project.save_data(task_output_filename, df_task.to_csv(index=False), overwrite=True)\n",
    "    \n",
    "    # export cleaned text\n",
    "    df_text_for_word2vec = df['CleanText']\n",
    "    text_for_word2vec_filename = 'text_for_word2vec_' + num + '.txt'\n",
    "    project.save_data(text_for_word2vec_filename, df_text_for_word2vec.to_csv(index=False), overwrite=True)\n",
    "    \n",
    "print('---DONE---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output 1 \"df_task\" : to be merged with skill dataset (structured data). Variable \"BGTJobId\" is a identifier.\n",
    "- (Phai) : Not completely sure if \"JobId\" is important or not -- so, let's keep it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BGTJobId</th>\n",
       "      <th>JobId</th>\n",
       "      <th>t_nr_analytic</th>\n",
       "      <th>t_nr_interactive</th>\n",
       "      <th>t_r_cognitive</th>\n",
       "      <th>t_r_manual</th>\n",
       "      <th>t_nr_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152166</th>\n",
       "      <td>37996970028</td>\n",
       "      <td>0xf5c763c5db981558e6528ba1f10c522c</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79487</th>\n",
       "      <td>37996570967</td>\n",
       "      <td>ad9b833a5b28c84e4da6871d9a61ef24fc338bc9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144155</th>\n",
       "      <td>37996908958</td>\n",
       "      <td>3532559d9a8a2b8a8c414889753bec1e2946b2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41144</th>\n",
       "      <td>37996393724</td>\n",
       "      <td>ad4943d78082dbf1de1ea54c56b665c957731e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30631</th>\n",
       "      <td>37996360124</td>\n",
       "      <td>734d5c91876629db1d24641a57a16eda9f4f5e</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57754</th>\n",
       "      <td>37996446131</td>\n",
       "      <td>4cf323cb24a8773a2548c628e545ff437ca04e78</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87116</th>\n",
       "      <td>37996596510</td>\n",
       "      <td>f892676436488e28797ad196448ccb168032df78</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79671</th>\n",
       "      <td>37996572944</td>\n",
       "      <td>709ce2bd373981178fa9f46562c66e43742c9b71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130883</th>\n",
       "      <td>37996856678</td>\n",
       "      <td>43e34aa3241285aeae81c5364a7855db63b6963d</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>37996536763</td>\n",
       "      <td>3598f217185b24ea5a57aaa7958ca6787a3b6a</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           BGTJobId                                     JobId  t_nr_analytic  \\\n",
       "152166  37996970028        0xf5c763c5db981558e6528ba1f10c522c              5   \n",
       "79487   37996570967  ad9b833a5b28c84e4da6871d9a61ef24fc338bc9              0   \n",
       "144155  37996908958    3532559d9a8a2b8a8c414889753bec1e2946b2              2   \n",
       "41144   37996393724    ad4943d78082dbf1de1ea54c56b665c957731e              0   \n",
       "30631   37996360124    734d5c91876629db1d24641a57a16eda9f4f5e              4   \n",
       "57754   37996446131  4cf323cb24a8773a2548c628e545ff437ca04e78              5   \n",
       "87116   37996596510  f892676436488e28797ad196448ccb168032df78              4   \n",
       "79671   37996572944  709ce2bd373981178fa9f46562c66e43742c9b71              0   \n",
       "130883  37996856678  43e34aa3241285aeae81c5364a7855db63b6963d              1   \n",
       "69998   37996536763    3598f217185b24ea5a57aaa7958ca6787a3b6a              5   \n",
       "\n",
       "        t_nr_interactive  t_r_cognitive  t_r_manual  t_nr_manual  \n",
       "152166                 4              0           0            0  \n",
       "79487                  0              0           0            0  \n",
       "144155                 5              0           0            1  \n",
       "41144                  0              0           3            1  \n",
       "30631                  0              1           1            0  \n",
       "57754                  3              0           0            2  \n",
       "87116                  1              1           5            2  \n",
       "79671                  0              0           0            0  \n",
       "130883                 0              0           0            1  \n",
       "69998                  5              0           0            1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output 2 \"df_text_for_word2vec\" : To be used to contruct word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152166    title store seasonal employee cashier company ...\n",
       "79487                                        host hostesses\n",
       "144155    tru green sales representative s base pay comm...\n",
       "41144     pharmacy tech driver elderwood other williamsv...\n",
       "30631     java developer collateral management company t...\n",
       "57754     personal trainer at hour fitness in sandy ut m...\n",
       "87116     kelly services alignment technician at pierce ...\n",
       "79671                               dental insurance humana\n",
       "130883    dependency case manager trainee cma brevard em...\n",
       "69998     headquarters mission street suite san francisc...\n",
       "Name: CleanText, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_for_word2vec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
